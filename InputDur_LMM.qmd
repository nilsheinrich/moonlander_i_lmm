---
title: "Nils Wendel Heinrich: Input Duration"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-27"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description
2 Covariates (continuous variables we believe affect the predicted variable) - N_visible_obstacles & N_visible_drift_tiles
1 Fixed Effect (categorical variable) - input noise

Doug: All of them (continuous and categorical variables) are called covariates. There are discrete (categorical) and numerical covariates.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000

const AoG = AlgebraOfGraphics;
```

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_InputData.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# We will filter out inputs that lasted less than 2 frames (no duration) and longer than 2.45s (controlling for blind input)
my_data = my_data[(my_data.input_duration .> 0.033) .& (my_data.input_duration .< 2.45), :]

describe(my_data)
```

### Contrasts

We will declare **ID** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

# Modeling input duration
We will log transform the predicted variable within the formula defined in the linear models.

Varying intercepts for **ID**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

#issingular(m_varyingInt1) # NOT overparameterized
VarCorr(m_varyingInt1)
#last(m_varyingInt1.λ)

```
0.060151 / (0.060151 + 0.442267) = 0.11972301947780534 as ICC for ID. We will keep ID as random effect and now explore random slope effects.

## most complex model
Including all fixed effects term in the random effects structure.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlop = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)  # NOT overparameterized

```

Leaving interaction term out of random effects
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    varSlop = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referring to BIC, no interaction is favored (AIC however favors the complex model). Proceeding with m_varyingSlope_complex_ni.

Stating zero correlation within random effects structure
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlop = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC favors _complex_zc and therefore correlation between random effects (AIC favors correlation though). Proceeding with m_varyingSlope_complex_zc and starting to delete individual random effects.

Deleting input noise
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    varSlop = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope1]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope1)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC (and AIC) favor keeping input noise as random effect.

Deleting N_visible_obstacles instead
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    varSlop = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope2]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope2)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
N_visible_obstacles should be kept as well. Proceeding with m_varyingSlope_complex_zc.

Deleting N_visible_drift_tiles
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    varSlop = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope3]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope3)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC and AIC agree on keeping N_visible_drift_tiles as random slope effect.

## Model selection
We can finally take a look at the main effects:
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlop = @formula(log(input_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

m_varyingSlope_complex_zc
```

## Caterpillar plot

```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```
This plot hints towards no correlation between the random effects.

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_zc)

```

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```

Taking a look at the distributions of the estimates for the main effects (leaving out intercept...)
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Input duration")
```

**Main results**
  Fixed-effects parameters:
────────────────────────────────────────────────────────────────────────────────
                                             Coef.  Std. Error       z  Pr(>|z|)
────────────────────────────────────────────────────────────────────────────────
(Intercept)                            -1.86558     0.0551709   -33.81    <1e-99
N_visible_obstacles                     0.0175964   0.00276096    6.37    <1e-09
N_visible_drift_tiles                   0.401293    0.0196813    20.39    <1e-91
input_noise: weak-none                  0.00104319  0.0130811     0.08    0.9364
input_noise: strong-weak               -0.0182364   0.0157568    -1.16    0.2471
N_visible_obstacles & N_visible_drift_tiles  
                                       -0.0167938   0.00147854  -11.36    <1e-29
────────────────────────────────────────────────────────────────────────────────

95% CI:
 β1  │ -1.97146    -1.75766
 β2  │ 0.012423    0.0230782
 β3  │ 0.362223    0.439877
 β4  │ -0.0235664  0.0275838
 β5  │ -0.0487669  0.0124872
 β6  │ -0.0197519  -0.0140028

Strangely more obstacles on the screen lead to longer input durations. I would have thought otherwise.

# Discussion
In a post-hoc analysis (eye_movement_analysis/input_data_analysis.ipynb), we find that in 0.8419563459983832  (84.2%) of drift cases, participants countered drift by steering against the applied drift. That is well above chance and might reflect a general strategy to deal with environmental drift. Something that we don't find for input noise.

Here, we only considered drifts that are below the agent (currently applying or about to apply) and visible.
