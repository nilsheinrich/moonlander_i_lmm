---
title: "Nils Wendel Heinrich: Progressive Saccades"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-26"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description
2 Covariates (continuous variables we believe affect the predicted variable) - N_visible_obstacles & N_visible_drift_tiles
1 Fixed Effect (categorical variable) - input noise

Doug: All of them (continuous and categorical variables) are called covariates. There are discrete (categorical) and numerical covariates.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000
```

```{julia}
const AoG = AlgebraOfGraphics;
```

Possible random effects: **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_ProgressiveSaccades.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering saccades with no amplitude
my_data = my_data[(my_data.saccade_amplitude .> 0), :]

describe(my_data)
```

### Contrasts

We will declare **ID** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
#my_cake = Dict(
#  :ID => Grouping(),
#  :input_noise => EffectsCoding(; levels=["N", "W", "S"]),
#  :Test => HypothesisCoding(
#    [
#      -1 +1 0
#      -1 0 +1
#    ];
#    levels=["N", "W", "S"],
#    labels=["No_inputNoise", "Weak_inputNoise", "Strong_inputNoise"],
#  ),
#);

my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

# Modeling saccade amplitude
We will log transform the predicted variable within the formula stated in the LMMs.

## Building various models

### Only varying intercept LMM

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1)
VarCorr(m_varyingInt1)

```
ID will be kept as random effect. We are good to go and explore random slope effects.

### Exploring random effects structure of the model
We start by building the most complex random effects structure around ID (just dumping all of the fixed effects in the varying slope). 

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)  # NOT overparameterized
```
We will compare the complex model with one without the interaction term within the random slope effects.

```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni)
VarCorr(m_varyingSlope_complex_ni)
last(m_varyingSlope_complex_ni.λ)  # no zeroes on the diagonal.

```
Testing both complex models against each other:

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referring to BIC (AIC they are the same), the model without the interaction term wins. We will therefore proceed with that one.

#### Building models of less complexity
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)
VarCorr(m_varyingSlope_complex_zc)
last(m_varyingSlope_complex_zc.λ)

```

Throwing _complex and _complex_zc against each other.
```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_ni, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_ni, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc is favored. We will proceed with this one.

### Deleting individual random effects
Throwing input noise out of the random effects structure.
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC favors the complex model: m_varyingSlope_complex_zc.

Throwing out N_visible_obstacles:
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)

```

Throwing models against each other.
```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc is still favored by BIC.

Kicking out N_visible_drift_tiles:
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc prevails. We will refer to that model for hypothesis testing.

## Model selection
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(saccade_amplitude) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

m_varyingSlope_complex_zc

```

## Principal component analysis

Won't give us anything of value, because we stated zercorr.
```{julia}

MixedModels.PCA(m_varyingSlope_complex_zc)

```

## Caterpillar plot

```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```
This plot hints towards no correlation between the random effects.

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_zc)

```
We see some shrinkage happening. Here we borrow strength in terms of applying the linear trend that can be found in the general data to individual data points that didn't show this linear trend. 

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```

Taking a look at the distributions of the estimates for the main effects (leaving out intercept...)
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Saccade amplitude (progressive saccades)")
```

Zooming in on the interaction effect.
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_visible_obstacles*N_visible_drift_tiles"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

We see significance only for the **N_visible_obstacles** (negative), meaning this covariate significantly **decreases** saccade amplitude in progressive saccades.

# Sanity check: Modeling saccade landing site distance to spaceship

## Explore random effects

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1)
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```
We will keep ID as random effect and explore random slope effects.

## Building models
Building the most complex model:
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)
VarCorr(m_varyingSlope_complex)
last(m_varyingSlope_complex.λ)

```

### Models of reduced complexity

Ditching interaction term within random effects structure
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni)
#VarCorr(m_varyingSlope_complex_ni)
#last(m_varyingSlope_complex_ni.λ)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_ni, :m_varyingSlope_complex]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_ni, m_varyingSlope_complex)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referring to BIC, the model without the interaction term m_varyingSlope_complex_ni is favored. Proceeding with this one.

Not allowing correlation between random effects.
```{julia}
#| label: m_varyingSlope_zc

m_varyingSlope_zc = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc)
#VarCorr(m_varyingSlope_zc)
#last(m_varyingSlope_zc.λ)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_ni, :m_varyingSlope_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_ni, m_varyingSlope_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Zero correlation between random effects is favored when referring to BIC. Proceeding with zc and ditching individual random effects.

Leaving out input noise
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)
#VarCorr(m_varyingSlope1)
#last(m_varyingSlope1.λ)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_zc is still favored. We will keep input noise in the random effects structure.

Leaving out N_visible_obstacles
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)
#VarCorr(m_varyingSlope2)
#last(m_varyingSlope2.λ)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_zc wins.

Leaving out N_visible_drift_tiles instead.
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)
#VarCorr(m_varyingSlope3)
#last(m_varyingSlope3.λ)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_zc wins again. It seems that this is the best model for describing the available data. Therefore we will refer to m_varyingSlope_zc for hypothesis testing.

## Model selection
```{julia}
#| label: m_varyingSlope_zc

m_varyingSlope_zc = let
    formula = @formula(saccLandSite_dist_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

m_varyingSlope_zc

```

## Caterpillar plot

The caterpillar plot allows for visually validating no correlation between random effects.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope_complex
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_zc)

```
Again shrinkage happens to data points that didn't show a strong linear trend before.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_zc)
tbl = samples.tbl
```

### Plotting
Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```

Native ridgeplot
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to agent (progressive saccades)")
```

Zooming in on the interaction effect.
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_visible_obstacles*N_visible_drift_tiles"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

When modeling saccade landing site in terms of distance to spaceship, we find significant effects for N_visible_obstacles, N_visible_drift_tiles, and their interaction effect. We however find **no** effect for either level of input noise. Therefore we can derive that input noise does not affect the distance to the spaceship in saccade landing sites in progressive saccades.

# Modeling distance to closest obstacle of saccade landing site

## Explore random effects

### Including N_visible_obstacles as random intercept effect!!!!

```{julia}

my_cake = Dict(
  :ID => Grouping(),
  :N_visible_obstacles => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

Including both, ID and N_visible_obstacles as random intercept effect.
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(saccLandSite_dist_to_closestObstacle ~ 1 + N_visible_drift_tiles + input_noise + 
    (1 | ID) + 
    (1 | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # NOT overparameterized

```

### Exploring random slopes
Simply dumping the fixed effects structure into the random slopes.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(saccLandSite_dist_to_closestObstacle ~ 1 + N_visible_drift_tiles + input_noise + 
    (1 + N_visible_drift_tiles + input_noise | ID) + 
    (1 + N_visible_drift_tiles + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # Not overparameterized

```
This is the most complex model that we can build. We will now start to eliminate individual terms and try to reduce model complexity.

Stating zero correlation between random effects:
```{julia}
#| label: m_varyingSlope_zc

m_varyingSlope_zc = let
    formula = @formula(saccLandSite_dist_to_closestObstacle ~ 1 + N_visible_drift_tiles + input_noise + 
    zerocorr(1 + N_visible_drift_tiles + input_noise | ID) + 
    zerocorr(1 + N_visible_drift_tiles + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
In this case, **no** correlation between random slopes is favored (BIC, bit not AIC). Proceeding with m_varyingSlope_zc.

Deleting input_noise
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(saccLandSite_dist_to_closestObstacle ~ 1 + N_visible_drift_tiles + input_noise + 
    zerocorr(1 + N_visible_drift_tiles | ID) + 
    zerocorr(1 + N_visible_drift_tiles | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_zc, :m_varyingSlope1]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_zc, m_varyingSlope1)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Input_noise should be kept as random slope effect.

Deleting N_visible_drift_tiles instead...
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(saccLandSite_dist_to_closestObstacle ~ 1 + N_visible_drift_tiles + input_noise + 
    zerocorr(1 + input_noise | ID) + 
    zerocorr(1 + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_zc, :m_varyingSlope2]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_zc, m_varyingSlope2)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Nope, referring to BIC (or AIC), the complex model with N_visible_drift_tiles as random slope wins.

## Model selection
```{julia}
#| label: m_varyingSlope_zc

m_varyingSlope_zc = let
    formula = @formula(saccLandSite_dist_to_closestObstacle ~ 1 + N_visible_drift_tiles + input_noise + 
    zerocorr(1 + N_visible_drift_tiles + input_noise | ID) + 
    zerocorr(1 + N_visible_drift_tiles + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

m_varyingSlope_zc

```

## Caterpillar plot

We can check the correlation between the random slopes by looking at the caterpillar.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope_complex
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```
And visually it looks like zerocorr is not valid, but it was the better model...

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_zc)

```
Again shrinkage happens to data points that didn't show a strong linear trend before.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_zc)
tbl = samples.tbl
```

### Plotting
Taking a look at the distributions of the estimates for the main effects:

Let's first take a look into the bounds
```{julia}
confint(samples)
```

Native ridgeplot
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to closest obstacle (progressive saccades)")
```
We find no significant effects for N_visible_drift_tiles or either level of input noise (our hypothesis).

