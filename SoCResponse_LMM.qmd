---
title: "Nils Wendel Heinrich: SoC Responses"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-27"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description
Covariates (continuous variables):
    - N_prior_crashs
    - N_consecutive_crash_success
    (- trials_since_last_crash)
    (- crashed_in_last_trial)
Fixed Effects (categorical variables):
    - done
    - level_difficulty
    - drift
    - input noise


# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000
```

```{julia}
const AoG = AlgebraOfGraphics;
```

One possible random effect: **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_SoCData.arrow"))
#my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering saccades with no amplitude
#my_data = my_data[(my_data.saccade_amplitude .> 0), :]

# new variable: level_difficulty based on level
# 1 & 2: easy
# 3 & 4: medium
# 5 & 6: hard


describe(my_data)
```

### Contrasts

We will declare **ID** a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  #:input_noise => EffectsCoding(; levels=["N", "W", "S"]),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      -1 0 +1
    ];
    levels=["N", "W", "S"],
    labels=["No_inputNoise", "Weak_inputNoise", "Strong_inputNoise"],
  ),
);

#contrasts = Dict(:input_noise => EffectsCoding())
```

## Building various models

### Only varying intercept LMM

Varying intercepts for **ID**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_prior_crashs + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```
The random intercept model hints towards ID being a valid random effect. Proceeding by including random slope effects.

### Most complex model
Simply dumping all fixed effect terms into the random effects structure.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_prior_crashs + (1 + done + level_difficulty * drift + input_noise + N_prior_crashs | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized
```
The model is too complex. Will start by deleting interaction term from random effects structure.

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_prior_crashs + (1 + done + level_difficulty + drift + input_noise + N_prior_crashs | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized
```
Still overparameterized.

Stating zerocorr
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_prior_crashs + zerocorr(1 + done + level_difficulty + drift + input_noise + N_prior_crashs | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized
```
Not better.

Only entering *done* as random slope.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_prior_crashs + (1 + done | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)
```
N_prior_crashs significantly increases SoC rating. Investigating further...

Maybe it has to with crashing and subsequently succeeding. This might boost agency or control feeling. Therefore switching from N_prior_crashs to N_consecutive_crash_success. 

```{julia}
#| label: m_varyingSlope_ccs

m_varyingSlope_ccs = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + done | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_ccs)
VarCorr(m_varyingSlope_ccs)
last(m_varyingSlope_ccs.λ)
```

### Exploring random effects structure

```{julia}
#| label: m_varyingSlope_ccs_1

m_varyingSlope_ccs_1 = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_ccs_1)  # overparameterized
```

Eliminate interaction effect
```{julia}
#| label: m_varyingSlope_ccs_ni

m_varyingSlope_ccs_ni = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + done + level_difficulty + drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_ccs_ni)  # overparameterized
```

Also leaving out drift
```{julia}
#| label: m_varyingSlope_ccs_

m_varyingSlope_ccs_ = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + done + level_difficulty + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_ccs_)  # overparameterized
```

level_difficulty thrown out.
```{julia}
#| label: m_varyingSlope_ccs_

m_varyingSlope_ccs_ = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + done + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_ccs_)  # overparameterized
```

Eliminating done...
- overparamterized

input_noise alone results in singularity...

Only done and N_consecutive_crash_success in combination did not produce singularity.
```{julia}
#| label: m_varyingSlope_ccs_2

m_varyingSlope_ccs_2 = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + done + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_ccs_2)  # Not overparameterized
VarCorr(m_varyingSlope_ccs_2)
last(m_varyingSlope_ccs_2.λ)
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_ccs, :m_varyingSlope_ccs_2]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_ccs, m_varyingSlope_ccs_2)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
The more complex model, m_varyingSlope_ccs_2, is favored when referring to BIC (or AIC).

Trying to include input_noise in random slope effects.
```{julia}
#| label: m_varyingSlope_ccs_3

m_varyingSlope_ccs_3 = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + done + N_consecutive_crash_success + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_ccs_3)  # overparameterized
```
No chance... Proceeding to do hypothesis testing on m_varyingSlope_ccs_2

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_ccs_2)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI")
```

Zooming in on input_noise:Strong effect.
```{julia}
plt = data(tbl) * mapping(
  [:β08] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "input_noise:Strong"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Zooming in on the N_consecutive_crash_success effect.
```{julia}
plt = data(tbl) * mapping(
  [:β09] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_consecutive_crash_success"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
**Discussing the results:** We find significant effects for done, both levels of level_difficulty, drift, (not for input_noise:None - compared to grand mean; as well as input_noise:Weak - which is compared to level None) input_noise:Strong (compared to level :Weak), N_consecutive_crash_success, and not for the interaction effect of level_difficulty:hard and drift (:medium and drift interaction not bootstrapped?).

