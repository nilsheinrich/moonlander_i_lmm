---
title: "Nils Wendel Heinrich: SoC Responses"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-27"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description
Covariates (continuous variables):
    - N_prior_crashs
    - N_consecutive_crash_success
    (- trials_since_last_crash)
    (- crashed_in_last_trial)
    - N_fixations
    - N_saccades
Fixed Effects (categorical variables):
    - done
    - level_difficulty
    - drift
    - input noise

We will predict SoC judgement rating. Responses were given on a 7-step Likert scale. We will use parametric statistics assuming that the tests are sufficiently robust for this type of data.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000

const AoG = AlgebraOfGraphics;
```

One possible random effect: **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_SoCData.arrow"))
#my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering saccades with no amplitude
#my_data = my_data[(my_data.saccade_amplitude .> 0), :]

# new variable: level_difficulty based on level
# 1 & 2: easy
# 3 & 4: medium
# 5 & 6: hard


describe(my_data)
```

### Contrasts

We will declare **ID** a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);

#contrasts = Dict(:input_noise => EffectsCoding())
```

## Building various models

### Only varying intercept LMM

Already starting with N_consecutive_crash_success...

Varying intercepts for **ID**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```
The random intercept model hints towards ID being a valid random effect. Proceeding by including random slope effects.

### Most complex model
Simply dumping all fixed effect terms into the random effects structure.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + (1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized
```
The model is too complex. Will start by deleting interaction term from random effects structure.

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + (1 + done + level_difficulty + drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized
```
That didn't do the trick. 

Stating zero correlation:
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + done + level_difficulty + drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # overparamterized
```
Still detecting singularity.

Starting to delete single random slope effects while keeping zerocorr: 
N_fixations
N_fixations + N_saccades
N_fixations + N_saccades + N_consecutive_crash_success
input_noise + N_fixations + N_saccades + N_consecutive_crash_success
level_difficulty + input_noise + N_fixations + N_saccades + N_consecutive_crash_success (worked)

```{julia}
#| label: m_varyingSlope_zc

m_varyingSlope_zc = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + done + drift | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc) # NOT overparamterized
```

### Further exploring random effects structure

Deleting drift as well:
```{julia}
#| label: m_varyingSlope_zc_done

m_varyingSlope_zc_done = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + 
    zerocorr(1 + done | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_done) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_zc, :m_varyingSlope_zc_done]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_zc, m_varyingSlope_zc_done)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Deleting drift as random slope effect results in a higher BIC (and AIC).

Deleting done instead:
```{julia}
#| label: m_varyingSlope_zc_drift

m_varyingSlope_zc_drift = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + 
    zerocorr(1 + drift | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_drift) # NOT overparamterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_zc, :m_varyingSlope_zc_drift]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_zc, m_varyingSlope_zc_drift)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Drift and done as random slope effects is favoured against only drift (by BIC and AIC).

#### Testing against other single random slope effects

```{julia}
#| label: m_varyingSlope_zc_input_noise

m_varyingSlope_zc_input_noise = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_input_noise) # singularity
```

```{julia}
#| label: m_varyingSlope_zc_ld

m_varyingSlope_zc_ld = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + level_difficulty | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_ld) # singularity
```

```{julia}
#| label: m_varyingSlope_zc_Nfix

m_varyingSlope_zc_Nfix = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + N_fixations | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_Nfix) # NO singularity
```

```{julia}
#| label: m_varyingSlope_zc_Nsacc

m_varyingSlope_zc_Nsacc = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + N_saccades | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_Nsacc) # singularity
```

```{julia}
#| label: m_varyingSlope_zc_ccs

m_varyingSlope_zc_ccs = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + N_consecutive_crash_success | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_zc_ccs) # NO singularity
```

Throwing viable models against each other.
```{julia}

gof_summary = let
  nms = [:m_varyingSlope_zc, :m_varyingSlope_zc_ccs]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_zc, m_varyingSlope_zc_ccs)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Only N_consecutive_crash_success as random slope looses against done+drift.

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_zc, :m_varyingSlope_zc_Nfix]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_zc, m_varyingSlope_zc_Nfix)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Again done+drift reaches smaller BIC and AIC.

## Model selection
```{julia}
#| label: m_varyingSlope_zc

m_varyingSlope_zc = let
    varSlope = @formula(SoC ~ 1 + done + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + zerocorr(1 + done + drift | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

```

## Bootstrapping
```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_zc)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="SoC judgements")
```

#### Zooming in on individual PDFs

level_difficulty: medium looks close
```{julia}
plt = data(tbl) * mapping(
  [:β03] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "level_difficulty: medium"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
But is significant!

N_fixations.
```{julia}
plt = data(tbl) * mapping(
  [:β08] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_fixations"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
Really close (looking at confint) but significant.

Zooming in on the N_saccades effect.
```{julia}
plt = data(tbl) * mapping(
  [:β09] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_saccades"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
Not significant

N_consecutive_crash_success:
```{julia}
plt = data(tbl) * mapping(
  [:β10] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_consecutive_crash_success"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
Looks good!

**Discussing the results:** We find significant effects for (stating 95% CIs):
- done (1.1896, 1.82211)
- level_difficulty: hard (-0.597581, -0.220188), not medium though
- drift (-0.495813, -0.0276936)
- input_noise: strong vs. weak (-0.470881, -0.237453), not weak vs. none though
- N_fixations (8.38616e-6, 0.00145474), not for N_saccades though
- N_consecutive_crash_success (0.0187794, 0.0454039)
- ld: hard * drift (-0.970736, -0.46804)
- ld: medium * drift (-0.536937, -0.0194473)
### Frome these findings, we will only report done and will further include this covariate as random effect.

# Including DONE as random effect because we are not interested in variance caused by success
random effects coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :done => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);

```

only random intercept model
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(SoC ~ 1 + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + 
    (1 | ID) + 
    (1 | done));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```
done is suitable for a random effect. Next up, exploring random slopes.

## Exploring random slope effects
As soon as we entered a random slope effect for done, we detected singularity. We will therefore introduce random slope effects solely for ID.
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    varSlop = @formula(SoC ~ 1 + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + 
    (1 + drift| ID) + 
    (1 | done));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1) # Not overparamterized

```

```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    varSlop = @formula(SoC ~ 1 + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + 
    (1 + N_fixations| ID) + 
    (1 | done));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2) # Not overparamterized

```

```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    varSlop = @formula(SoC ~ 1 + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + 
    (1 + N_consecutive_crash_success| ID) + 
    (1 | done));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3) # Not overparamterized

```

Testing models against each other:
```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope1, :m_varyingSlope2, :m_varyingSlope3]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope1, m_varyingSlope2, m_varyingSlope3)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```

m_varyingSlope1 reached lowest BIC (and AIC). We will therefore proceed with drift as random slope effect for ID.

## Model selection
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    varSlop = @formula(SoC ~ 1 + level_difficulty * drift + input_noise + N_fixations + N_saccades + N_consecutive_crash_success + 
    (1 + drift| ID) + 
    (1 | done));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

#m_varyingSlope1 = let
#    varSlop = @formula(SoC ~ 1 + level_difficulty * drift + input_noise + #N_consecutive_crash_success + 
#    (1 + drift| ID) + 
#    (1 | done));
#    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
#  end

m_varyingSlope1

```

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope1)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="SoC judgements")
```

**Discussing the results:** We find significant effects for (stating 95% CIs):

### with N_fixations & N_saccades **PAPER**:
- level_difficulty: hard (-0.642159, -0.23268), not medium though
- drift did **not** ultimately reach significance (-0.535484, 0.0266281).
- input_noise: strong vs. weak (-0.459791, -0.21568), not weak vs. none though
- N_fixations (0.00043501, 0.00196481)
- N_consecutive_crash_success (0.0123833, 0.0409701)
- ld: hard * drift (-0.971484, -0.42742), no significance for ld: medium * drift

### without N_fixations & N_saccades:
- level_difficulty: hard (-0.639921, -0.225046), not medium though
- drift did **not** ultimately reach significance (-0.367976, 0.0402688).
- input_noise: strong vs. weak (-0.463515, -0.217886), not weak vs. none though (-0.0809001, 0.168251)
- N_consecutive_crash_success (0.013414, 0.0411281)
- ld: hard * drift (-0.97185, -0.425968), no significance for ld: medium * drift (-0.53702, 0.0249987)
