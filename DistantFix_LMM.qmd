---
title: "Nils Wendel Heinrich: Distant Fixations"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-12"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---
# Helpful
Shift + Control + ´ (right of ß) to close and open terminal...

# Description
2 Covariates (continuous variables we believe affect the predicted variable) - N_visible_obstacles & N_visible_drift_tiles
1 Fixed Effect (categorical variable) - input noise

Doug: All of them (continuous and categorical variables) are called covariates. There are discrete (categorical) and numerical covariates.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000
```

```{julia}
const AoG = AlgebraOfGraphics;
```

# Modeling fixation duration

## Code book
Possible random effects: only **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_DistantFixations.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering saccades with no amplitude
my_data = my_data[(my_data.fixation_duration .>= 0.06), :]

describe(my_data)
```

### Contrasts

We will declare **ID** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

# Modeling fixation duration

## Building various models

### Only varying intercept LMM

Varying intercept only for **ID**:
```{julia}
#| label: m_varyingInt2

m_varyingInt2 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt2)
```

### Exploring random effects structure of the model
 We start by building the most complex random effects structure around ID (just dumping all of the fixed effects in the varying slope). 

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)  # NOT overparameterized
#VarCorr(m_varyingSlope_complex)
#last(m_varyingSlope_complex.λ)  # we only have one random effect: ID, but last() puts it into a nice matrix
# no zeroes on the diagonal
```

Deleting the interaction term within the random effects structure.
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni)  # NOT overparameterized
#VarCorr(m_varyingSlope_complex_ni)
#last(m_varyingSlope_complex_ni.λ)  # we only have one random effect: ID, but last() puts it into a nice matrix
# no zeroes on the diagonal
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
The complex model with the interaction (m_varyingSlope_complex) is favored when referring to BIC (or AIC).

Build the complex model but without correlations between random effects.
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
No correlation between random effects is favored (by BIC, not by AIC though). Will proceed with m_varyingSlope_complex_zc...

### Building various models with varying slope of less complexity by throwing out random effects

leaving out input noise completely:
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
m_varyingSlope_complex_zc is favored by BIC (agreeing with AIC).

Leaving out N_visible_drift_tiles instead:
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC (and AIC) favors m_varyingSlope_complex_zc.

Leaving out N_visible_obstacles:
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc is still favored by BIC (or AIC). Therefore proceeding with hypothesis testing using this model.

## Model selection
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)

```

## Principal component analysis

```{julia}

MixedModels.PCA(m_varyingSlope_complex_zc)

```

In the output, we will look at the normalized cumulative variances (second table). Each PC is focused individually. Do the loads make sense? For example, are loads high for visible stuff and low for inputNoise? 
**we don't see anything of interest because we stated zerocorr**

## Caterpillar plot

We can visually check for having no correlation between random effects with a caterpillar plot.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot
This plot shows where strength was borrowed and applied to the data (in terms of adjusting linear trends).
```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_zc)

```

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

```{julia}
confint(samples)
```

The ridgeplot will show us the estimates and their distributions (as plotted above). We will omit the intercept because it would zoom out too much.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Fixation duration (distant fixations)")
```
We only see a significant effect (increase) for **N_visible_obstacles** on fixation duration in distant fixations.

Zooming in on the interaction effect.
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles*N_drift_tiles"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

**Discussing the results:** Only **N_visible_obstacles** affetcs the fixation duration in distant fixations (0.00333352, 0.0183757).

# Modeling fixation location - distance to spaceship

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # NOT overparameterized

```

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # singular

```
We detect singularity. We will adjust the random effects by ditching the interaction term.

```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # NOT overparameterized

```
That did the trick. Proceeding from here...

```{julia}
#| label: m_varyingSlope_complex_ni

VarCorr(m_varyingSlope_complex_ni)

```
All of the covariates in the random effects structure show sufficient correlation. We will further adjust the random effects structure of the model for a final model selection. 

Stating zero correlation between random effects
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
Close but, zero correlation is not favored by BIC (nor by AIC). Therefore. proceeding with m_varyingSlope_complex_ni.

### Deleting individual random effects

Deleting input noise
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
BIC (and AIC) favors keeping input noise as random effect.

N_visible_drift_tiles and input_noise; kicking N_visible_obstacles out.
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)
VarCorr(m_varyingSlope2)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
N_visible_drift_tiles can also be kept as random effect.

N_visible_obstacles and input_noise; leaving out N_visible_drift_tiles.
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)
VarCorr(m_varyingSlope3)

```

Throwing models against each other:
```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC favors model m_varyingSlope_complex_ni. We'll stick with this one.

## Model selection
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # NOT overparameterized

```
Now we can actually take a closer look at the main effects.

Via a PCA, we can check whether loadings of our various main effects on the different principle components make sense. 
```{julia}
#m_varyingSlope4
MixedModels.PCA(m_varyingSlope_complex_ni)
```

## Caterpillar plot

We can confirm the correlation between random effects visually by looking at the caterpillar:
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_ni));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_ni)

```
Not much shrinkage happening. Meaning we already had a good linear trend across the data.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_ni)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Calling confint will give you the bounds, when visually it might be hard to verify significance of effects...
```{julia}
confint(samples)
```
**N_visible_obstacles** is significant.

Now let's plot the bounds (without intercept) to visualize when 0 is within the bounds (meaning no significance). It's basically the plot above for the beta estimates but every estimate gets its own row, which makes it easier to read.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to agent (distant fixations)")
```

Zooming in on the interaction effect:
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles*N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
95% CI is not intersecting with x=0-line.

We find **no** significance for either level of input noise. We however find that **N_visible_obstacles** is significantly **decreasing** distance to the spaceship within exploring fixations (-0.139299, -0.0431337). No interaction effect found.

# Modeling fixation location - distance to closest obstacle
Controlling for the number of visible obstacles.

```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :N_visible_obstacles => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

Varying intercepts for **ID** and **N_visible_obstacles**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + (1 | ID) 
    + (1 | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # NOT overparameterized

```
Both can be kept as random intercepts.

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_drift_tiles + input_noise | ID) 
    + (1 + N_visible_drift_tiles + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # singular

```
We detect singularity. First thing we can do is to state zerocorr.

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles + input_noise | ID) 
    + zerocorr(1 + N_visible_drift_tiles + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # singular

```
Still overparameterized. We will start to ditch individual random slopes.

Only keeping input_noise as random slope:
```{julia}
#| label: m_varyingSlope_in

m_varyingSlope_in = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + input_noise | ID) 
    + zerocorr(1 + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_in) # singular

```

Only keeping N_drift_tiles as random slope:
```{julia}
#| label: m_varyingSlope_d_zc

m_varyingSlope_d_zc = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles | ID) 
    + zerocorr(1 + N_visible_drift_tiles | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_d_zc) # NOT overparameterized

```
This one works. We can test it against our varyingInt model:

```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_d_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_d_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referring to BIC (or AIC), the varyingSlope model wins.

Allowing correlation between random effects again (without stating zerocorr):
```{julia}
#| label: m_varyingSlope_d

m_varyingSlope_d = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_drift_tiles | ID) 
    + (1 + N_visible_drift_tiles | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_d) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_d, :m_varyingSlope_d_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_d, m_varyingSlope_d_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referrin to BIC, the zerocorr model wins (not when referring to AIC though). We will use the zerocorr model for hypothesis testing.

## Model selection
```{julia}
#| label: m_varyingSlope_d_zc

m_varyingSlope_d_zc = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles | ID) 
    + zerocorr(1 + N_visible_drift_tiles | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_d_zc) # NOT overparameterized

```

## Caterpillar plot
We can visually verify having stated zero correlation between random effects.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope_complex
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_d_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot
```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_d_zc)

```
Some points were aggressively adjusted.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_d_zc)
tbl = samples.tbl
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```
We already see that input noise does not significantly influences the distance to the closest obstacle in distant fixations. The number of visible drift tiles does though.

### Plotting
Taking a look at the distributions of the estimates for the main effects:

initial glimpse, leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer(["N_drift", "input_noiseW", "input_noiseS",])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to closest obstacle (distant fixations)")
```

**N_visible_drift_tiles** significantly increases the distance to the closest obstacle in distant fixations (0.0234815, 0.0762633). Either level of input noise has no effect on the predicted variable.
