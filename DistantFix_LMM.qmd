---
title: "Nils Wendel Heinrich: Exploring Fixations"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-12"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---
# Helpful
Shift + Control + ´ (right of ß) to close and open terminal...

# Description
2 Covariates (continuous variables we believe affect the predicted variable) - N_visible_obstacles & N_visible_drift_tiles
1 Fixed Effect (categorical variable) - input noise

Doug: All of them (continuous and categorical variables) are called covariates. There are discrete (categorical) and numerical covariates.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000
```

```{julia}
const AoG = AlgebraOfGraphics;
```

# Modeling fixation duration

## Code book
Two possible random effects: **ID** (the subject itself) and **total_control_loss** (whether subject reported having lost all control in specific situations).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_ExploringFixations.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering saccades with no amplitude
my_data = my_data[(my_data.fixation_duration .> 0), :]

describe(my_data)
```

### Contrasts

We will declare **ID** and **total_control_loss** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :total_control_loss => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

# Modeling fixation duration

## Building various models

### Only varying intercept LMM

Varying intercepts for **ID** and **total_control_loss**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID) + (1 | total_control_loss));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # singular

```
We detect singularity. One of the random intercept effects does not explain variance within the data to a sufficient amount. Expecting total_control_loss to cause an issue here.

```{julia}

#first: ID
last(m_varyingInt1.λ)  # for total_control_loss: 0.0
VarCorr(m_varyingInt1)

```
We will ditch total_control_loss from here on.


Varying intercept only for **ID**:
```{julia}
#| label: m_varyingInt2

m_varyingInt2 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt2)
```

### Exploring random effects structure of the model
 We start by building the most complex random effects structure around ID (just dumping all of the fixed effects in the varying slope). 

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)  # NOT overparameterized
#VarCorr(m_varyingSlope_complex)
#last(m_varyingSlope_complex.λ)  # we only have one random effect: ID, but last() puts it into a nice matrix
# no zeroes on the diagonal
```

Deleting the interaction term within the random effects structure.
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni)  # NOT overparameterized
#VarCorr(m_varyingSlope_complex_ni)
#last(m_varyingSlope_complex_ni.λ)  # we only have one random effect: ID, but last() puts it into a nice matrix
# no zeroes on the diagonal
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
The complex model with the interaction (m_varyingSlope_complex) is favored when referring to BIC (or AIC).

Build the complex model but without correlations between random effects.
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
No correlation between random effects is favored (by BIC, not by AIC though). Will proceed with m_varyingSlope_complex_zc...

### Building various models with varying slope of less complexity by throwing out random effects

leaving out input noise completely:
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
m_varyingSlope_complex_zc is favored by BIC (agreeing with AIC).

Leaving out N_visible_drift_tiles instead:
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC (and AIC) favors m_varyingSlope_complex_zc.

Leaving out N_visible_obstacles:
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc is still favored by BIC (or AIC). Therefore proceeding with hypothesis testing using this model.

## Principal component analysis

```{julia}

MixedModels.PCA(m_varyingSlope_complex_zc)

```

In the output, we will look at the normalized cumulative variances (second table). Each PC is focused individually. Do the loads make sense? For example, are loads high for visible stuff and low for inputNoise? 
**we don't see anything of interest because we stated zerocorr**

## Caterpillar plot

We can visually check for having no correlation between random effects with a caterpillar plot.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot
This plot shows where strength was borrowed and applied to the data (in terms of adjusting linear trends).
```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_zc)

```

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

```{julia}
confint(samples)
```

The ridgeplot will show us the estimates and their distributions (as plotted above). We will omit the intercept because it would zoom out too much.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Fixation duration (distant fixations)")
```
We only see a significant effect (increase) for **N_visible_obstacles** on fixation duration in distant fixations.

Zooming in on the interaction effect.
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles*N_drift_tiles"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

# Modeling fixation location

Varying intercepts for **ID** and **total_control_loss**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID) + (1 | total_control_loss));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # singular

```
We detect singularity. One of the random intercept effects does not explain variance within the data to a sufficient amount. Expecting total_control_loss to be the issue here.

```{julia}

#first: ID
last(m_varyingInt1.λ)  # for total_control_loss: 0.0
VarCorr(m_varyingInt1)

```
Again total_control_loss can be neglected for the random effects structure

```{julia}
#| label: m_varyingInt2

m_varyingInt2 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt2) # NOT overparamterized

```

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # singular

```
We detect singularity. We will adjust the random effects by ditching the interaction term.

```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # NOT overparamterized

```
That did the trick. Proceeding from here...

```{julia}
#| label: m_varyingSlope_complex_ni

VarCorr(m_varyingSlope_complex_ni)

```
All of the covariates in the random effects structure show sufficient correlation. We will further adjust the random effects structure of the model for a final model selection. 

Stating zero correlation between random effects
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # NOT overparamterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
Close but, zero correlation is not favored by BIC (nor by AIC). Therefore. proceeding with m_varyingSlope_complex_ni.

### Deleting individual random effects

Deleting input noise
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
BIC (and AIC) favors keeping input noise as random effect.

N_visible_drift_tiles and input_noise; kicking N_visible_obstacles out.
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)
VarCorr(m_varyingSlope2)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
N_visible_drift_tiles can also be kept as random effect.

N_visible_obstacles and input_noise; leaving out N_visible_drift_tiles.
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)
VarCorr(m_varyingSlope3)

```

Throwing models against each other:
```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC favors model m_varyingSlope_complex_ni. We'll stick with this one.

## Model selection
Now we can actually take a closer look at the main effects.

Via a PCA, we can check whether loadings of our various main effects on the different principle components make sense. 
```{julia}
#m_varyingSlope4
MixedModels.PCA(m_varyingSlope_complex_ni)
```

## Caterpillar plot

We can confirm the correlation between random effects visually by looking at the caterpillar:
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_ni));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_ni)

```
Not much shrinkage happening. Meaning we already had a good linear trend across the data.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_ni)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Calling confint will give you the bounds, when visually it might be hard to verify significance of effects...
```{julia}
confint(samples)
```
**N_visible_obstacles**, **N_visible_drift_tiles**, and their **interaction** term are significant.

Now let's plot the bounds (without intercept) to visualize when 0 is within the bounds (meaning no significance). It's basically the plot above for the beta estimates but every estimate gets its own row, which makes it easier to read.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to agent (distant fixations)")
```

Zooming in on the interaction effect:
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles*N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
95% CI is not intersecting with x=0-line.

We find **no** significance for either level of input noise. We however find that **N_visible_obstacles** and **N_visible_drift_tiles** are significantly **decreasing** distance to the spaceship within exploring fixations and that the **interaction** of both significantly **increases** the effects of both main effects. The interaction effect tells us that the effect of N_visible_obstacles depends on the level of N_visible_drift_tiles and vice versa. 
