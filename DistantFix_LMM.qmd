---
title: "Nils Wendel Heinrich: Distant Fixations"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-12"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---
# Helpful
Shift + Control + ´ (right of ß) to close and open terminal...

# Description
2 Covariates (continuous variables we believe affect the predicted variable) - N_visible_obstacles & N_visible_drift_tiles
1 Fixed Effect (categorical variable) - input noise

Doug: All of them (continuous and categorical variables) are called covariates. There are discrete (categorical) and numerical covariates.

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000

const AoG = AlgebraOfGraphics;
```

# Modeling fixation duration

## Code book
Possible random effects: only **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_DistantFixations.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering fixations with duration less than 25 samples
# fixdur >= 0.0125
my_data = my_data[(my_data.fixation_duration .>= 0.0125), :]  # 0.06
#eliminating fixations outside of game boarders
my_data = my_data[(my_data.distance_to_spaceship .< 16.63762484977781), :]

describe(my_data)
```

### Contrasts

We will declare **ID** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

# Modeling fixation duration

## Building various models

### Only varying intercept LMM

Varying intercept only for **ID**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1)
```

### Exploring random effects structure of the model
 We start by building the most complex random effects structure around ID (just dumping all of the fixed effects in the varying slope). 

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)  # Not overparameterized
#VarCorr(m_varyingSlope_complex)
#last(m_varyingSlope_complex.λ)  # we only have one random effect: ID, but last() puts it into a nice matrix
# no zeroes on the diagonal
```

Deleting the interaction term within the random effects structure.
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni)  # Not overparameterized
#VarCorr(m_varyingSlope_complex_ni)
#last(m_varyingSlope_complex_ni.λ)  # we only have one random effect: ID, but last() puts it into a nice matrix
# no zeroes on the diagonal
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_ni wins in terms of BIC (not AIC though).

Build the complex_ni model but without correlations between random effects.
```{julia}
#| label: m_varyingSlope_complex_ni_zc

m_varyingSlope_complex_ni_zc = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni_zc)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_ni_zc, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_ni_zc, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_ni_zc is better when referring to BIC (and AIC). Proceeding with m_varyingSlope_complex_ni_zc.

Putting interaction term back in.
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_complex_ni_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_complex_ni_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc has lower BIC (and AIC). Proceeding with m_varyingSlope_complex_zc.

### Building various models with varying slope of less complexity by throwing out random effects

leaving out input noise completely:
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
m_varyingSlope_complex_zc is favored by BIC (agreeing with AIC).

Leaving out N_visible_drift_tiles instead:
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC (and also AIC) favors m_varyingSlope_complex_zc.

Leaving out N_visible_obstacles:
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)  # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc is still favored by BIC (or AIC). Therefore proceeding with hypothesis testing using this model.

## Model selection
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)

```

## Principal component analysis

```{julia}

MixedModels.PCA(m_varyingSlope_complex_zc)

```

In the output, we will look at the normalized cumulative variances (second table). Each PC is focused individually. Do the loads make sense? For example, are loads high for visible stuff and low for inputNoise? 
**we don't see anything of interest because we stated zerocorr**

## Caterpillar plot

We can visually check for having no correlation between random effects with a caterpillar plot.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot
This plot shows where strength was borrowed and applied to the data (in terms of adjusting linear trends).
```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_zc)

```

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

```{julia}
confint(samples)
```

The ridgeplot will show us the estimates and their distributions (as plotted above). We will omit the intercept because it would zoom out too much.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Fixation duration (distant fixations)")
```

Zooming in on the interaction effect.
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles*N_drift_tiles"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Results:
**N_visible_obstacles** significantly increases fixation duration (0.000327827, 0.02224).

# Modeling fixation location - distance to spaceship

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # NOT overparameterized

```

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # Not overparameterized

```

no interaction term in random slopes
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + 
    (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # Not overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
m_varyingSlope_complex_ni wins slightly (lower BIC; not agreeing AIC). Proceeding with m_varyingSlope_complex_ni.

Stating zero correlation between random effects
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # NOT overparameterized

```
That did the trick.

```{julia}
#| label: m_varyingSlope_complex_zc_ni

m_varyingSlope_complex_zc_ni = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc_ni) # NOT overparameterized

```


```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_complex_zc_ni, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_complex_zc_ni, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
m_varyingSlope_complex_zc (zero correlation with interaction term) is favored by BIC (and by AIC). Therefore. proceeding with m_varyingSlope_complex_zc.

### Deleting individual random effects

Deleting input noise
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
BIC (and AIC) favors keeping input noise as random effect.

N_visible_drift_tiles and input_noise; kicking N_visible_obstacles out.
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)
#VarCorr(m_varyingSlope2)

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end
```
N_visible_drift_tiles can also be kept as random effect.

N_visible_obstacles and input_noise; leaving out N_visible_drift_tiles.
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)
#VarCorr(m_varyingSlope3)

```

Throwing models against each other:
```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC favors model m_varyingSlope_complex_zc. We'll stick with this one.

## Model selection
```{julia}
#| label: selected model

m_varyingSlope_complex_zc = let
    formula = @formula(1/distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # NOT overparameterized

```
Now we can actually take a closer look at the main effects.

Via a PCA, we can check whether loadings of our various main effects on the different principle components make sense. 
```{julia}
#m_varyingSlope4
MixedModels.PCA(m_varyingSlope_complex_zc)
```

## Caterpillar plot

We can confirm having stated zero correlation between random effects visually by looking at the caterpillar:
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_zc)

```
Some shrinkage is happening. This is where the strong linear tendency is borrowed.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```
"Confidence intervals are obtained using a parametric bootstrap with N replicates."

### Plotting
Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Calling confint will give you the bounds, when visually it might be hard to verify significance of effects...
```{julia}
confint(samples)
```

Now let's plot the bounds (without intercept) to visualize when 0 is within the bounds (meaning no significance). It's basically the plot above for the beta estimates but every estimate gets its own row, which makes it easier to read.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to agent (distant fixations)")
```

Zooming in on the interaction effect:
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles*N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
95% CI is not intersecting with x=0-line.

We find **no** significance for either level of input noise. We see a significant positive effect for **N_visible_obstacles** (0.00081265, 0.00149221).

# Modeling fixation location - distance to closest obstacle
Controlling for the number of visible obstacles.

```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :N_visible_obstacles => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

Varying intercepts for **ID** and **N_visible_obstacles**:
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + (1 | ID) 
    + (1 | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # NOT overparameterized

```
Both can be kept as random intercepts.

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_drift_tiles + input_noise | ID) 
    + (1 + N_visible_drift_tiles + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # singular

```
We detect singularity. First thing we can do is to state zerocorr.

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles + input_noise | ID) 
    + zerocorr(1 + N_visible_drift_tiles + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # singular

```
Still overparameterized. We will start to ditch individual random slopes.

Only keeping input_noise as random slope:
```{julia}
#| label: m_varyingSlope_in

m_varyingSlope_in = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + input_noise | ID) 
    + zerocorr(1 + input_noise | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_in) # singular

```
Still singular.

Only keeping N_drift_tiles as random slope:
```{julia}
#| label: m_varyingSlope_d_zc

m_varyingSlope_d_zc = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles | ID) 
    + zerocorr(1 + N_visible_drift_tiles | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_d_zc) # NOT overparameterized

```
This one works. We can test it against our varyingInt model:

```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_d_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_d_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referring to BIC (or AIC), the varyingSlope model wins.

Allowing correlation between random effects again (without stating zerocorr):
```{julia}
#| label: m_varyingSlope_d

m_varyingSlope_d = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_drift_tiles | ID) 
    + (1 + N_visible_drift_tiles | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_d) # Not overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_d, :m_varyingSlope_d_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_d, m_varyingSlope_d_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```

## Model selection
Going with m_varyingSlope_d_zc because increasing model complexity will only lead to overparameterization.
```{julia}
#| label: selected model

m_varyingSlope_d_zc = let
    formula = @formula(log(Dist_to_closest_obstacles) ~ 1 + N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles | ID) 
    + zerocorr(1 + N_visible_drift_tiles | N_visible_obstacles));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_d_zc) # NOT overparameterized

```

## Caterpillar plot
We can visually verify having stated zero correlation between random effects.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for the selected model
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_d_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_d_zc)
tbl = samples.tbl
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```
We already see that input noise does not significantly influences the distance to the closest obstacle in distant fixations. The number of visible drift tiles does though.

### Plotting
Taking a look at the distributions of the estimates for the main effects:

initial glimpse, leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer(["N_drift", "input_noiseW", "input_noiseS",])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to closest obstacle (distant fixations)")
```

**N_visible_drift_tiles** significantly increases the distance to the closest obstacle in distant fixations (0.00119341, 0.0733309).


# Modeling horizontal-only distance to spaceship
Not focusing on the time horizon of action goals but rather how many motor actions are needed to implement the action goal, solely testing the horizontal distance to the spaceship should give hints towards the planning of motor action in response to environmental features drift & input noise.

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_DistantFixations_test.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering fixations with duration less than 25 samples
# fixdur >= 0.0125
my_data = my_data[(my_data.fixation_duration .>= 0.0125), :]  # 0.06
#eliminating fixations outside of game boarders
my_data = my_data[(my_data.distance_to_spaceship .< 16.63762484977781), :]

describe(my_data)
```

### Contrasts

We will declare **ID** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

Box-Cox suggests log transform
```{julia}
#| label: m_1

m_1 = let
    formula = @formula(log(horizontal_distance_to_spaceship) ~ 1 + N_visible_obstacles + N_visible_drift_tiles + input_noise 
    + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_1) # NOT overparameterized

```

```{julia}
#| label: m_complex

m_complex = let
    formula = @formula(log(horizontal_distance_to_spaceship) ~ 1 + N_visible_obstacles + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_complex) # NOT overparameterized

```

```{julia}
#| label: m_2

m_2 = let
    formula = @formula(log(horizontal_distance_to_spaceship) ~ 1 + N_visible_obstacles + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_obstacles + N_visible_drift_tiles | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_2) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_complex, :m_2]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_complex, m_2)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_complex is better.

```{julia}
#| label: m_3

m_3 = let
    formula = @formula(log(horizontal_distance_to_spaceship) ~ 1 + N_visible_obstacles + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_3) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_complex, :m_3]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_complex, m_3)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_complex is still better.

```{julia}
#| label: m_4

m_4 = let
    formula = @formula(log(horizontal_distance_to_spaceship) ~ 1 + N_visible_obstacles + N_visible_drift_tiles + input_noise 
    + (1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_4) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_complex, :m_4]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_complex, m_4)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_complex is still still better.

Taking a look at selected model:
```{julia}
m_complex
```

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_complex)
tbl = samples.tbl
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```

```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Horizontal-only Distance to Spaceship")
```

### Discussing the results
We find a single significant effect for the **N_visible_drift_tiles**. Any additional drift section on screen increases the horizontal distance to the spaceship (0.283584, 0.463759).
