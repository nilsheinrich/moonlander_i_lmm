---
title: "Nils Wendel Heinrich: Sanity check"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-10-23"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

## Packages
```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000

const AoG = AlgebraOfGraphics;
```

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_SoCData.arrow"))
# 1425 rows

# Filtering runs in which players crashed
my_data = filter(row -> row.done, my_data)
# y_data = filter(row -> !row.done, my_data) # for only crashs

# new variable: level_difficulty based on level
# 1 & 2: easy
# 3 & 4: medium
# 5 & 6: hard

# convert boolean column done to int (o vs. 1)
my_data[!,:done] = convert.(Int,my_data[!,:done])

describe(my_data)
```
we will only consider successful runs. Crashs may lead in a strong variance in N_fixations.

# Predicting total number of fixations during trial

### Contrasts

We will declare **ID** as grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding

```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
  :level_difficutly => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["easy", "medium", "hard"],
    labels=["medium-easy", "hard-medium"],
  ),
);

```

## Building models

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```
Seems like ID is a viable random effect. Next we will explore random slope effects.

### Model of highest complexity
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    (1 + level_difficulty * drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized

```
We detect singularity. We will go ahead an delete the interaction terms within the random effects structure.

```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    (1 + level_difficulty + drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # overparamterized

```
Still singular. Stating zerocorr next.

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + level_difficulty + drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # overparamterized

```
That didn't solve singularity. Starting to delete individual random slope effects.

Deleted:
- N_consecutive_crash_success
- input_noise + N_consecutive_crash_success
- drift + input_noise + N_consecutive_crash_success
... detected singularity every single time.

Entering a single slope effect:
- level_difficulty
- input_noise
- N_consecutive_crash_success
does not work,

```{julia}
#| label: m_varyingSlope_

m_varyingSlope_ = let
    formula = @formula(N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    (1 + drift | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_) # overparamterized

```

works:
- drift

```{julia}
#| label: m_varyingSlope_d

m_varyingSlope_d = let
    formula = @formula(N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + drift | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_d) # NOT overparamterized

```

Throwing model against only varying intercept model:
```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_d]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_d)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Now it gets tricky: referring to BIC m_varyingInt1 wins, but referring to AIC, the model with the drift random slope wins... Going for BIC.


### Model selection
```{julia}
#| label: main_effects

m_varyingInt1 = let
    formula = @formula(N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
m_varyingInt1

```
Nothing reaches significance. Running bootstrap

## Bootstrapping
```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingInt1)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI")
```
No significant effect found. N_fixations cannot be predicted with the covariates at hand.

# Predicting ratio of distant fixations

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```
ID is a suitable random effect. A huge portion of the total variance is caused by ID.

## Exploring random slope effects

Dumping all possible covariates in the random slope effects structure.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + level_difficulty * drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized

```
We detect singularity. The first thing we can do to reduce complexity is eliminating the interaction term in the random slopes.

```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 + level_difficulty + drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # overparamterized

```
That didn't cut it. 

Stating zerocorr
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + level_difficulty + drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # overparamterized

```
Nope still singular. Starting to cut individual random slopes.

Eliminated:
- level_difficulty
- input_noise (works)
- drift
- N_consecutive_crash_success

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + level_difficulty + drift + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # NOT overparamterized

```

Only when input_noise was not included in the random slopes, the model didn't show singularity. Including **level_difficulty + drift + N_consecutive_crash_success** is therefore our most complex model. Proceed from here

Throwing out level_difficulty
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + drift + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1) # NOT overparamterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope1]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope1)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
BIC (and AIC) favors neglecting level_difficulty as random slope. Proceeding with Slope1

Throwing out drift
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + level_difficulty + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2) # NOT overparamterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope2]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope2)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Slope2 wins in terms of BIC (and AIC). Proceeding with m_varyingSlope2.

Throwing out N_consecutive_crash_success.
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + level_difficulty + drift | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3) # NOT overparamterized

```

We cannot throw Slope3 against Slope2 because the test would have a resulting Chisq of =0. But we can test it against Slope1:
```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope1]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope1)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
And Slope1 wins. But Slope1 looses against Slope2. We can therefore infer that Slope2 is the best fitting model. We will now proceed with keeping only **single random slope effects**.

Keeping only level_difficulty:
```{julia}
#| label: m_varyingSlope_simple1

m_varyingSlope_simple1 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + level_difficulty | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_simple1) # NOT overparamterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_simple1]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_simple1)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Slope2 is favored by BIC (and AIC).

Keeping only N_consecutive_crash_success:
```{julia}
#| label: m_varyingSlope_simple2

m_varyingSlope_simple2 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_simple2) # NOT overparamterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_simple2]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_simple2)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referring to BIC, the model with only N_consecutive_crash_success as random slope is favored (not when referring to AIC though).

Now only keeping drift as random slope
```{julia}
#| label: m_varyingSlope_simple3

m_varyingSlope_simple3 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + drift | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_simple3) # NOT overparamterized

```

But we cannot throw the model against _simple2 because the resulting Chisq would be =0. We will therefore test it against Slope2.
```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_simple3]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_simple3)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
And we see that Slope2 wins. But it lost against _simple2. We can therefore infer that _simple2 is the superior model.

Testing against m_varyingInt1:
```{julia}

gof_summary = let
  nms = [:m_varyingInt1, :m_varyingSlope_simple2]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingInt1, m_varyingSlope_simple2)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
_simple2 is favored by BIC (and AIC). We found our model that best describes the data.

## Model selection
Sticking to m_varyingSlope_simple2. 
```{julia}

m_varyingSlope_simple2 = let
    formula = @formula(N_distant_fixations/N_fixations ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_simple2) # NOT overparamterized

m_varyingSlope_simple2
```
1 main effect is significant: level_difficulty: hard & drift -interaction

## Bootstrapping
```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_simple2)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI")
```
Results:
- **level_difficulty: hard & drift** -interaction is significant (-0.130991, -0.00859757). This tells us that the main effects of **level_difficulty: hard** and **drift** are interacting negatively with each other. But these aren't significant...


# Predicting N_saccades

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(N_saccades ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```

## Exploring random slope effects

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(N_saccades ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    (1 + level_difficulty * drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized

```
We detect singularity. Proceeding by deleting interaction term within random effects structure.

```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(N_saccades ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    (1 + level_difficulty + drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # overparamterized

```
Still overparameterized. Stating zerocorr.

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(N_saccades ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + level_difficulty + drift + input_noise + N_consecutive_crash_success | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # overparamterized

```
Didn't solve it. Proceeding by deleting individual random slope effects.

Deleted:
- N_consecutive_crash_success
- input_noise
- drift
- level_difficulty

- level_difficulty + N_consecutive_crash_success
- ... nothing works

```{julia}
#| label: m_varyingSlope_

m_varyingSlope_ = let
    formula = @formula(N_saccades ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + 
    zerocorr(1 + ? | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_) # overparamterized
#VarCorr(m_varyingSlope_)
#last(m_varyingSlope_.λ)

```

## Model selection
Sticking to m_varyingInt1. 
```{julia}

m_varyingInt1 = let
    formula = @formula(N_saccades ~ 1 + level_difficulty * drift + input_noise + N_consecutive_crash_success + (1 | ID));
    fit(MixedModel, formula, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
m_varyingInt1

```
No main effect is significant.

## Bootstrapping
```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingInt1)
tbl = samples.tbl
```

```{julia}
confint(samples)
```

Visualizing 95% CIs individually for every covariate.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI")
```
No significant effect found. N_saccades cannot be predicted with the covariates at hand.


# Predicting done (True=success vs. False=crash) with level_difficulty, level length, and input_noise

New data now also with crashs:
```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_SoCData.arrow"))
# 1425 rows

# new variable: level_difficulty based on level
# 1 & 2: easy
# 3 & 4: medium
# 5 & 6: hard

# convert boolean column done to int (o vs. 1)
my_data[!,:done] = convert.(Int,my_data[!,:done])

describe(my_data)
```

```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
  :level_difficutly => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["easy", "medium", "hard"],
    labels=["medium-easy", "hard-medium"],
  ),
);

```

The predicted varibale **done** is binary. We will therefore define an object *dist* set to the Bernoulli distribution.
```{julia}
#| label: binary_outcome_variable

dist = Bernoulli()

```

## Building models

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingInt1) # Not overparamterized
VarCorr(m_varyingInt1)
last(m_varyingInt1.λ)

```
Seems like ID is a viable random effect. Next we will explore random slope effects.

### Exploring random slope effects
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + 
    (1 + level_difficulty * drift + input_noise | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # overparamterized

```

Deleting the interaction term.
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + 
    (1 + level_difficulty + drift + input_noise | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # overparamterized

```

Going to get rid of correlations between random slopes.
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + 
    zerocorr(1 + level_difficulty + drift + input_noise | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # overparamterized

```

#### Starting to delete individual random slopes

Deleting drift...
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + 
    zerocorr(1 + level_difficulty + input_noise | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingSlope1) # overparamterized

```

Deleting input_noise instead...
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + 
    zerocorr(1 + level_difficulty + drift | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingSlope2) # overparamterized

```

Instead deleting level_difficulty...
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + 
    zerocorr(1 + drift + input_noise | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingSlope3) # overparamterized

```

No way keeping two of the factors as random slopes. We will try including only a single random slope.

```{julia}
#| label: m_varyingSlope_

m_varyingSlope_ = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + 
    zerocorr(1 + drift | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

issingular(m_varyingSlope_) # overparamterized

```
Including only a single random slope always results in singularity. Proceeding with the only random intercept model, m_varyingInt1.

## Model selection
```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    formula = @formula(done ~ 1 + level_difficulty * drift + input_noise + (1 | ID));
    fit(MixedModel, formula, my_data, dist; contrasts=my_cake);
  end

```

```{julia}
VarCorr(m_varyingInt1)
```
There is no residual variance estimated: this is because the Bernoulli distribution doesn't have a scale parameter.

```{julia}
m_varyingInt1
```

```{julia}
using Effects

# first filter data that cannot be predicted by model (missing cases)
filter!(!ismissing, [my_data.level_difficulty, my_data.drift, my_data.input_noise])

# specify levels of covariates for which we want predictions
covariate_levels = Dict(
  :level_difficulty => ["easy", "medium", "hard"], 
  :drift => ["false", "true"], 
  :input_noise => ["N", "W", "S"]
)

# generate predictions
preds = effects(covariate_levels, m_varyingInt1; invlink=AutoInvLink())
```
**Discussing the results:**
significant effects for:
- level_difficulty: hard (-3.63577, p<.00)
- level_difficulty: medium (-2.17203, p=0.0007)
- input_noise: strong-weak (-0.83504, p<.00), not for weak-none (-0.207822, p=0.2924)
- level_difficulty: hard & drift (-2.59284, p=0.0016), not for medium & drift (-1.09482, p=0.1913)

ultimately **no** effect of drift on the predicted variable (-0.278706, p=0.7257)


