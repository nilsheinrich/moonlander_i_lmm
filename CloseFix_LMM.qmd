---
title: "Nils Wendel Heinrich: Close Fixations"
subtitle: "Moonlander I - Analysis"
author: "Nils Wendel Heinrich"
date: "2023-09-25"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description
2 Covariates (continuous variables we believe affect the predicted variable) - N_visible_obstacles & N_visible_drift_tiles
1 Fixed Effect (categorical variable) - input noise

# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using AlgebraOfGraphics
using CairoMakie
using DataFrames
using DataFrameMacros
using MixedModels
using MixedModelsMakie
using Random
#using RCall

CairoMakie.activate!(; type="svg");
```

```{julia}
#| label: constants
const RNG = MersenneTwister(36)
N_iterations = 10000
```

```{julia}
const AoG = AlgebraOfGraphics;
```

# Modeling fixation duration

## Code book
Possible random effects: only **ID** (the subject itself).

```{julia}
#| label: data

my_data = DataFrame(Arrow.Table("data/Experiment1_CloseFixations.arrow"))
my_data = dropmissing(my_data, [:N_visible_obstacles, :N_visible_drift_tiles])

# Filtering fixations with duration less than 25 samples
# fixdur >= 0.0125
my_data = my_data[(my_data.fixation_duration .>= 0.06), :]

describe(my_data)
```

### Contrasts

We will declare **ID** as a grouping variable as well as define the effects coding for the discrete covariate input noise.

#### Hypothesis Coding
```{julia}
my_cake = Dict(
  :ID => Grouping(),
  :input_noise => HypothesisCoding(
    [
      -1 +1 0
      0 -1 +1
    ];
    levels=["N", "W", "S"],
    labels=["weak-none", "strong-weak"],
  ),
);
```

# Modeling fixation duration

## Building various models

### Only varying intercept LMM

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1)
VarCorr(m_varyingInt1)
```

### Exploring random effects structure of the model
 We start by building the most complex random effects structure around **ID** (just dumping all of the fixed effects in the varying slope). Including the interaction effect

```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlop = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex)  # NOT overparameterized
```
The model is not singular. We will test it against one without the interaction term.

```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    varSlop = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
When referring to BIC (or AIC), the model without the interaction term fits better. Proceeding with m_varyingSlope_complex_ni.

#### Models of less complexity

Stating zerocorr
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlop = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_complex_ni]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_complex_ni)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Not allowing correlation between random effects is favored by BIC (not by AIC though). Proceed with m_varyingSlope_complex_zc and starting to delete individual random effects.

Throwing input noise out of the random effects structure of the model.
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    varSlop = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
_complex_zc is favored by BIC (and AIC). Will proceed with that one.

Throwing out N_visbile_drift_tiles.
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    varSlop = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
_complex_zc is again favored by BIC (and AIC).

Throwing out N_visible_obstacles.
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    varSlop = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3)  # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Referring to BIC, _complex_zc is ahead. We will therefore continue with hypothesis testing using m_varyingSlope_complex_zc

### Model selection
Finally taking a look at the main effects within m_varyingSlope_complex_zc:
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlop = @formula(log(fixation_duration) ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + zerocorr(1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlop, my_data; contrasts=my_cake);
  end

VarCorr(m_varyingSlope_complex_zc)

```

Due to zerocorr, no valuable information (but for the sake of completion):
```{julia}

MixedModels.PCA(m_varyingSlope_complex_zc)

```

## Caterpillar plot
We can visually verify having stated zero correlation between random effects.
```{julia}
#| fig-cap1: Prediction intervals on subject random effects for model m_varyingSlope_complex
#| label: fig-cm_varyingSlope
#|
cm_varyingSlope = first(ranefinfo(m_varyingSlope_complex_zc));
caterpillar!(Figure(; resolution=(800, 1200)), cm_varyingSlope; orderby=1)
```

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage
#|
#| fig-cap: Shrinkage plots of the subject random effects in the chosen model
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_varyingSlope_complex_zc)

```
We see a few data points which are aggressively adjusted, especially for input noise W and S. Here is where strength was borrowed from somewhere else and applied to adjust the linear trends of these individual data points.

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```

### Plotting
Taking a look at the distributions of the estimates for the main effects:

initial glimpse, leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

Let's first take a look into the bounds
```{julia}
confint(samples)
```

Now let's plot the bounds (without intercept) to visualize when 0 is within the bounds (meaning no significance). It's basically the plot above for the beta estimates but every estimate gets its own row, which makes it easier to read.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Fixation duration (close fixations)")
```
We see significance for **N_visible_obstalces** and **N_visible_drift_tiles** and their **interaction** term. Input noise doesn't seem to significanty affect the fixation duration in close fixations.

Zooming in on the interaction effect.
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_visible_obstacles*N_visible_drift_tiles"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```
Of the fixed effects included in our model, significant effects are:
**N_visible_obstacles** (0.00392587, 0.0209003)
**N_visible_drift_tiles** (0.114914, 0.203967) 
**interaction** (-0.0194157, -0.0098614);
- interaction term tells us that the effect of **N_visible_drift_tiles** depends on **N_visible_obstacles** and vice versa.

# Modeling fixation location

## Modeling fixation location (in terms of distance to spaceship)

```{julia}
#| label: m_varyingInt1

m_varyingInt1 = let
    varInt = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 | ID));
    fit(MixedModel, varInt, my_data; contrasts=my_cake);
  end

issingular(m_varyingInt1) # NOT overparameterized

```
We're good to go now.

## Building various models with varying slope

starting with the most complex model.
```{julia}
#| label: m_varyingSlope_complex

m_varyingSlope_complex = let
    varSlope = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex) # NOT overparameterized
```

### Models of reduced complexity
```{julia}
#| label: m_varyingSlope_complex_ni

m_varyingSlope_complex_ni = let
    varSlope = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + (1 + N_visible_obstacles + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_ni) # NOT overparameterized
```

Throwing interaction and no interaction against each other:
```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_ni, :m_varyingSlope_complex]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_ni, m_varyingSlope_complex)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
_complex is favored by BIC (and by AIC).

Neglecting all correlations between random effects.
```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlope = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope_complex_zc) # NOT overparameterized
```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope_complex_zc, :m_varyingSlope_complex]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope_complex_zc, m_varyingSlope_complex)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
_complex_zc is slightly favored by BIC (not by AIC though). We will proceed with zercorr from here on.

#### Throwing out random effect terms

Leaving out input noise:
```{julia}
#| label: m_varyingSlope1

m_varyingSlope1 = let
    varSlope = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope1) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope1, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope1, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
_complex_zc is favored by BIC.

Leaving out N_visible_obstacles:
```{julia}
#| label: m_varyingSlope2

m_varyingSlope2 = let
    varSlope = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope2) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope2, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope2, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
Again, m_varyingSlope_complex_zc is favored.

Leaving out N_visible_drift_tiles:
```{julia}
#| label: m_varyingSlope3

m_varyingSlope3 = let
    varSlope = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise 
    + zerocorr(1 + N_visible_obstacles + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

issingular(m_varyingSlope3) # NOT overparameterized

```

```{julia}

gof_summary = let
  nms = [:m_varyingSlope3, :m_varyingSlope_complex_zc]
  mods = eval.(nms)
  lrt = MixedModels.likelihoodratiotest(m_varyingSlope3, m_varyingSlope_complex_zc)
  DataFrame(;
    name = nms, 
    dof=dof.(mods),
    deviance=round.(deviance.(mods), digits=0),
    AIC=round.(aic.(mods),digits=0),
    AICc=round.(aicc.(mods),digits=0),
    BIC=round.(bic.(mods),digits=0),
    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),
    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),
    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))
  )
end

```
m_varyingSlope_complex_zc prevails. We will therefore refer to _complex_zc for hypothesis testing.

## Model selection
Finally taking a look at the main effects within the selected model:

```{julia}
#| label: m_varyingSlope_complex_zc

m_varyingSlope_complex_zc = let
    varSlope = @formula(distance_to_spaceship ~ 1 + N_visible_obstacles * N_visible_drift_tiles + input_noise + 
    zerocorr(1 + N_visible_obstacles * N_visible_drift_tiles + input_noise | ID));
    fit(MixedModel, varSlope, my_data; contrasts=my_cake);
  end

m_varyingSlope_complex_zc
```

```{julia}
VarCorr(m_varyingSlope_complex_zc)
```

Omitting PCA due to having stated zerocorr...

## Bootstrapping

```{julia}
samples = parametricbootstrap(RNG, N_iterations, m_varyingSlope_complex_zc)
tbl = samples.tbl
```

### Plotting
```{julia}
plt = data(tbl) * mapping(:σ) * AoG.density()
draw(plt; axis=(;title="Parametric bootstrap estimates of σ"))
```

Taking a look at the distributions of the estimates for the main effects:

leaving out intercept...
```{julia}
plt = data(tbl) * mapping(
  [:β2, :β3, :β4, :β5, :β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_obstacles", "N_drift", "input_noiseW", "input_noiseS", "N_obstacles * N_drift"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

```{julia}
confint(samples)
```

The ridgeplot will show us the estimates and their distributions (as plotted above). We will omit the intercept because it would zoom out too much.
```{julia}
ridgeplot(samples; show_intercept=false, xlabel="Bootstrap density and 95%CI", title="Distance to agent (close fixations)")
```

Zooming in on the interaction effect
```{julia}
plt = data(tbl) * mapping(
  [:β6] .=> "Bootstrap replicates of main effect estimates";
  color=dims(1) => renamer([ "N_visible_obstacles*N_visible_drift_tiles"])
  ) * AoG.density()
draw(plt; figure=(;supertitle="Parametric bootstrap β estimates of variance components"))
```

We detect significant effects for 
**N_visible_obstacles** (0.073475, 0.0990656) and 
**N_visible_drift_tiles** (0.126059, 0.360188) individually but also for their interaction effect (both **negatively** influencing the effect of the other; 
-0.0461236, -0.0106101).
**input noise: strong** (in comparison to input_noise: weak) also significantly influences (decreases; -0.137928, -0.0192497) fixation location in resting fixations.
